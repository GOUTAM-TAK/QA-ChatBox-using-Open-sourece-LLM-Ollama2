{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a RAG application from scratch\n",
    "\n",
    "Here is a high-level overview of the system we want to build:\n",
    "\n",
    "<img src='images/system1.png' width=\"1200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading the environment variables we need to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# load_dotenv()\n",
    "\n",
    "# OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# This is the YouTube video we're going to use.\n",
    "YOUTUBE_VIDEO = \"https://www.youtube.com/watch?v=cdiD-9MMpb0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the model\n",
    "Let's define the LLM model that we'll use as part of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "# model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "model = Ollama(model=\"llama2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test the model by asking a simple question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'During the COVID-19 pandemic, which started in 2020 and lasted for several months, there was no MLB World Series played. The 2020 World Series was cancelled due to the pandemic, and it was not played until 2021. Therefore, no MLB team won the World Series during that time period.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"What MLB team won the World Series during the COVID-19 pandemic?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result from the model is an `AIMessage` instance containing the answer. We can extract this answer by chaining the model with an [output parser](https://python.langchain.com/docs/modules/model_io/output_parsers/).\n",
    "\n",
    "Here is what chaining the model with an output parser looks like:\n",
    "\n",
    "<img src='images/chain1.png' width=\"1200\">\n",
    "\n",
    "For this example, we'll use a simple `StrOutputParser` to extract the answer as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# parser = StrOutputParser()\n",
    "\n",
    "# chain = model | parser\n",
    "# chain.invoke(\"What MLB team won the World Series during the COVID-19 pandemic?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing prompt templates\n",
    "\n",
    "We want to provide the model with some context and the question. [Prompt templates](https://python.langchain.com/docs/modules/model_io/prompts/quick_start) are a simple way to define and reuse prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: \\nAnswer the question based on the context below. If you can\\'t \\nanswer the question, reply \"I don\\'t know\".\\n\\nContext: Mary\\'s sister is Susana\\n\\nQuestion: Who is Mary\\'s sister?\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you can't \n",
    "answer the question, reply \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt.format(context=\"Mary's sister is Susana\", question=\"Who is Mary's sister?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now chain the prompt with the model and the output parser.\n",
    "\n",
    "<img src='images/chain2.png' width=\"1200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Of course! Based on the context provided, Mary's sister is Susana.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model \n",
    "chain.invoke({\n",
    "    \"context\": \"Mary's sister is Susana\",\n",
    "    \"question\": \"Who is Mary's sister?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining chains\n",
    "\n",
    "We can combine different chains to create more complex workflows. For example, let's create a second chain that translates the answer from the first chain into a different language.\n",
    "\n",
    "Let's start by creating a new prompt template for the translation chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate {answer} to {language}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a new translation chain that combines the result from the first chain with the translation prompt.\n",
    "\n",
    "Here is what the new workflow looks like:\n",
    "\n",
    "<img src='images/chain3.png' width=\"1200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSure! Here\\'s the translation of \"How many sisters does Mary have?\" in Spanish:\\n\\n¿Cuántas hermanas tiene María?\\n\\nAnd the answer is: 1.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "translation_chain = (\n",
    "    {\"answer\": chain, \"language\": itemgetter(\"language\")} | translation_prompt | model \n",
    ")\n",
    "\n",
    "translation_chain.invoke(\n",
    "    {\n",
    "        \"context\": \"Mary's sister is Susana. She doesn't have any more siblings.\",\n",
    "        \"question\": \"How many sisters does Mary have?\",\n",
    "        \"language\": \"Spanish\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcribing the YouTube Video\n",
    "\n",
    "The context we want to send the model comes from a YouTube video. Let's download the video and transcribe it using [OpenAI's Whisper](https://openai.com/research/whisper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: whisper in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (1.1.10)\n",
      "Requirement already satisfied: six in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from whisper) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tempfile\n",
    "# import whisper\n",
    "# from pytube import YouTube\n",
    "\n",
    "\n",
    "# # Let's do this only if we haven't created the transcription file yet.\n",
    "# if not os.path.exists(\"transcription.txt\"):\n",
    "#     youtube = YouTube(YOUTUBE_VIDEO)\n",
    "#     audio = youtube.streams.filter(only_audio=True).first()\n",
    "\n",
    "#     # Let's load the base model. This is not the most accurate\n",
    "#     # model but it's fast.\n",
    "#     whisper_model = whisper.load_model(\"base\")\n",
    "\n",
    "#     with tempfile.TemporaryDirectory() as tmpdir:\n",
    "#         file = audio.download(output_path=tmpdir)\n",
    "#         transcription = whisper_model.transcribe(file, fp16=False)[\"text\"].strip()\n",
    "\n",
    "#         with open(\"transcription.txt\", \"w\") as file:\n",
    "#             file.write(transcription)\n",
    "\n",
    "with open(\"transcription.txt\", \"r\") as file:\n",
    "     transcription = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the transcription and display the first few characters to ensure everything works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Langchain is a decentralized language learning platform leveraging blockchain technology. It connect'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"transcription.txt\") as file:\n",
    "    transcription = file.read()\n",
    "\n",
    "transcription[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the entire transcription as context\n",
    "\n",
    "If we try to invoke the chain using the transcription as context, the model will return an error because the context is too long.\n",
    "\n",
    "Large Language Models support limitted context sizes. The video we are using is too long for the model to handle, so we need to find a different solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: Langchain is a decentralized language learning platform that leverages blockchain technology to connect language learners with native speakers, facilitating language exchanges and earning tokens through smart contracts. It also features reputation systems, AI assessments, communities, resources, cross-platform access, decentralized governance, security measures, partnerships, cultural exchange, lifelong learning, and support for endangered languages and VR experiences for immersive learning.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "chain.invoke({\n",
    "    \"context\": transcription,\n",
    "    \"question\": \"what is langchain ?\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the transcription\n",
    "\n",
    "Since we can't use the entire transcription as the context for the model, a potential solution is to split the transcription into smaller chunks. We can then invoke the model using only the relevant chunks to answer a particular question:\n",
    "\n",
    "<img src='images/system2.png' width=\"1200\">\n",
    "\n",
    "Let's start by loading the transcription in memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"00:00:00\\tLangston is a framework that allows you to build applications on top of llm or large language model in this crash course video we are going to go over all the basics of Lang chain and then we will build a restaurant either generator application using streamlit where you can input any cuisine Indian Mexican Etc and it will generate a fancy restaurant name along with the manual items first let us understand what is Lang chain and what kind of problem does it addressed when you are using chat GPD as an\\n\\n00:00:30\\tapplication internally it is making call to open AI API which internally uses any llm such as GPD 3.5 or 4. in this case cat GP itself is not an llm it is an application whereas GPD 3.54 these are large language models now let's say you want to build an application for restaurant idea generator where you give a Cuisine and then it will generate a fancy name such as Taco Temptation for Mexican and menu items as well let's say you give Indian Cuisine it will say Okay Curry Curry Palace or Sahara Palace for Arabic\\n\\n00:01:09\\talong with these menu items so this is a sample application which we are going to build but this is an llm based application and for this we can use the same architecture as jet GPT where we can directly call open AI API and here I have provided a screenshot of their main API so you can call it and you can get a behavior similar to chat GPT internally it will use GPD 3.5 or gpt4 model in this case once again restaurant idea generator is an application similar to chatgpt but internally you are using\\n\\n00:01:42\\topen AI API and llms now there are a couple of limitations of following this approach and by the way uh the reason I'm telling you this is nowadays there is a big boom in the industry where every business wants to build their own llm you would think why they can't use chat GPT because GPT has no access to your internal organization data so people want to build applications which are based on llm okay so there is a clear demand and clear boom in the industry for this and why do business not use this kind of architecture well\\n\\n00:02:17\\tthere are a couple of things to consider first of all calling open AI API has a cost associated with it for every thousand token they will charge point zero zero two dollar or something you can check open AI pricing page but there is a cost associated with it and if you're a startup who is having funding issues and you know your budget is limited this is going to be a bottleneck for you another thing is you might have noticed chat GPT doesn't answer latest question its knowledge is limited to September 2021 as of this video\\n\\n00:02:48\\trecording so if you want to incorporate some latest information let's say from Google Wikipedia or somewhere else you can't get that here the other issue is atlick is my own software development and data science company if I want to know how many employees joined last month chat GPT can't answer because because it doesn't have access to my own internal organization data so if you use this kind of architecture for building your application you will hit some roadblocks or you will rather have some\\n\\n00:03:19\\tlimitation and look open AI guys are pretty smart actually if they want they can address all of this but their stance is very clear we will provide foundational apis and building framework is something that other people should do and that's what happens see if you have just open AI API it is not enough to build llm therefore you need some kind of framework where you can call open AI a gpt3 gpd4 or maybe if you want to save the cost you call some open source models such as hugging phase Bloom there\\n\\n00:03:54\\tare so many models out there let's say you want to use them you don't want to spend money on open AI gpt3 model then this framework should provide that plug-in Play support you know where you can integrate to one of these models and your code kind of Remains the Same this framework should also provide integration with Google search Wikipedia or even integration with your own organizational databases so that the application can pull information from these various sources as well and this framework is launching that is what it\\n\\n00:04:28\\tdoes it's a framework that allows you to build applications using llm okay let's install line chain now and uh do some initial setup let us first create an account on open AI you can go to open a website click on login and create a login using Google or individual email credentials and once your login is created you will come to our dashboard so let me just show you so you go to open AI say login you're logged in click on API and then from your account you can go to your manage account and API keys\\n\\n00:05:08\\tyou will find a key here which will look something like SK hyphen something that is like a password so you need to use that key in our code for line chain you can also create a separate keys for separate projects so I have some client projects going on YouTube tutorials so for each of them I have a separate key in your case you can just use the one key whether you can generate a new key here as well so let's say you copy that key to some secure place after that you won't be able to access it here so you have to delete and create\\n\\n00:05:42\\ta new key okay so let's say you have that key ready with you uh here then you can just import OS model and then in OS module you can create a environment variable with that particular key if your key will be SK something okay in my case I have stored that key in one python file okay I don't want to share that key with all of you that is the reason and that python file looks something like this you know secret underscore key dot Pi uh it will have my own internal key I can have n number of keys here and I'm just importing that\\n\\n00:06:22\\tvariable here and just setting it here Ctrl enter so that thing is set now let's go to the terminal and install some modules so you're going to install Lang chain model that's number one and the second module you are going to install is called pip install open AI once you have installed those modules let's now import few important things from Lang chain uh we are going to import the llm called open AI now we are using open AI because open AI I know it costs some money but it is the best one uh if you want some other ones\\n\\n00:07:01\\tthen just just hit Tab and it will just show you hugging phase whatever the the other type of whatever other llms that it has available it will show you all of that we are right now happy with open Ai and then I will create my open AI model it has a variable called temperature now what temperature means is how creative you want your model to be so if the temperature is set to let's say zero it means it is very safe it is not taking any bets but if it is one it will take risk it might generate wrong\\n\\n00:07:41\\toutput but it is very creative at the same time I tend to set it to 0.6.7 things like that and now in that llm you can pass any questions so let's say I want to open a restaurant for Indian food and I want some fancy name for it I am not able to come up with that product name idea or restaurant name idea and let's see what this guy does and I typed in the same question in here also see I want to open a restaurant for Mexican food it told me this if you say Indian food it will tell you something else so we\\n\\n00:08:22\\tare using essentially the same concept here okay so here uh it says okay maharaja's Palace Cuisine uh if you say Italian food see the name sounds real as if it's an Italian restaurant so we have imported that open a class which created an llm and in the llm we are just passing a simple text now I don't want to keep on changing this same string so I will now go ahead and create something called a prom template so from Lang chain dot prompts you can import a prompt template and in that prompt template\\n\\n00:09:05\\tyou can pass some variables such as input variable what will be the input variable by the way variables it will be a cuisine and then the template that you want looks something like this so I'll just copy paste here and what we are doing is just changing that Italian Etc with that Cuisine variable and this template is called prompt template and let's say this is for a restaurant name that's why I'm saying name here uh and once that template is created you can just say prompt template name dot format\\n\\n00:09:49\\tand in that format you can pass cuisine as let's say Mexican and see I want to open a restaurant for Mexican food if you say Italian it will say Italian food this is more like a python string formatting you would be wondering why you don't use Python string formatting well let me just show you that using something called chain so we are going to use this concept of chain in line chain and it is one of the most important objects in in language in framework you can figure out from the name of the framework itself and we are\\n\\n00:10:27\\timporting llm chain and llm chain is essentially a very simple object where you are saying my llm is this whatever you created here okay that is my llm and my prompt is this prompt template and this is my chain and in the chain you can say chain dot run let's say I want to open a American restaurant see the All-American Grille and Bar so now here I don't have to pass the whole sentence I want to open a restaurant for this is that I just passed the cuisine the variable and it will just work every time Mexican\\n\\n00:11:14\\tsee so internally it is calling open AI API and we made that connection via this module here so if you are using hugging phase then you'll have to do the hugging phase setup and it will call hugging face okay here we are kind of paying that cost but by the way uh when you created that open API account you got five dollar free credit so you should be okay five dollar is more than enough for initial learning and exploration and after that if you like it you can go ahead and pay money so that is the\\n\\n00:11:45\\tsimple chain that we got here now let's look at something called a sequential chain uh so let me just explain the concept first so far what we did is we had this Gene for generating restaurant name and it was generating it but let's say for that restaurant you want to generate a manual item four manual items so you can have this second component or a second chain where you pass restaurant name as an input and it should give you the menu items that you should include in that restaurant so if it is Indian restaurant it will say\\n\\n00:12:19\\tpanitica mangolis Etc if it is Mexican it will say quesadilla burrito things like that this thing is called Simple sequential chain and let's uh code that up but just to clarify the idea here you have one input and one output and you can have intermediate steps where the input of the second step is the output of the first step it is as easy as that so here uh once again I'm generating everything from scratch I have generated the name chain the same way that we did before this is the exit copy paste of\\n\\n00:12:56\\tthe previous code so nothing fancy here and then we are going to create another chain and I'm just copy pasting just to save your time where the input is restaurant name and we are saying suggest me some food menu items for restaurant that so it is like saying this see uh you're you're saying I want to open a restaurant for Indian food suggest a fancy name for this only one name please so let's say you talk to chatgpt in chat GPT generated this now you are saying that generate food menu items for\\n\\n00:13:35\\tsaffron spice and then return it as a comma separated list see this this is what you want you you want to generate all this list and you have now two chains which we are going to do control enter and execute this code and now from link chain from Lang chain dot chains it will show you all kind of chains you are going to import a simple sequential chain okay and that simple sequential chain will contain this individual chains that we created and by the way the order matters here so this is a restaurant name chain and then\\n\\n00:14:24\\tyou have a full item chain and that's it and now you will say chain dot run let's say you want to generate it for Indian food you're getting a response here and you print response sometimes it takes time so you have to wait for few seconds but it will generate the menu items vegetables yummy huh you're probably getting uh water in your mouth let's do Mexican for all those Mexican food lovers so while this chain looks good it is generating those food menu items uh I am not getting the restaurant name as such\\n\\n00:15:10\\tbecause say for Mexican food it is saying all these item but what is the restaurant name that was that intermediate step here that was the intermediate step but in the output in the simple sequential chain it gives you just one output but I want the restaurant name and the menu items both for that we have to use a different chain called sequential chain and this sequential chain can have multiple input multiple outputs so I can just say okay give me a name for Indian restaurant which is vegan and then in the output I\\n\\n00:15:42\\tcan say give me a restaurant name and items both as an output all right so let's try something like this here I think this whole code kind of Remains the Same I'm just going to add one or two extra things here so see this is my first chain and the extra thing that I have added is output key so the output of the first chain is the restaurant name and the second Gene looks something like this where the output key is manual items okay and now let's create the simple sequence in chain so I am going to say\\n\\n00:16:23\\tfrom Lang chain dot chains import sequence searching we only used simple sequential chain this is a sequential chain which is little kind of generic so sequential chain will take what kind of parameters will it take well first of all it will say chains and these are the two chains I have and then my input variables you can specify all the input variables so input variables I'm not including that wagon Etc let's keep things simple all I want is to Output the remaining things are same okay so in the output variables I\\n\\n00:17:04\\tshould say that I want a restaurant name and the menu items as my output variable let's call this a chain and by the way when you run this chain you can't just say Mexican because you might have multiple input variables that's why you need to give a dictionary you will say Cuisine is let's say Arabic run not supported okay run is not yeah so run is not supported you have to just call it just like that no function just chain and then back it just give the argument then enter and see how much with pita\\n\\n00:17:52\\tbread Falafel and the name of the restaurant is the Arabian Bistro so it's giving in fact it's giving input as well so it's giving input and both the outputs whatever code we have written so far we will use that code and create extremely based application for restaurant name generator I am in my C code directory I have created this empty folder called restaurant name generator you see there are no files here and I'm going to Now launch pycharm which is a free python code editor and in this you can select open and you can open that\\n\\n00:18:29\\tparticular folder so I will go to my C code directory and in that I will locate restaurant name generator hit OK and it will create a I think empty main dot Pi file and I can just remove the content here and I'm going to just import a streamlit library first now if you don't know about streamlit it is a library that allows data scientists to build POC application proof of concept application simple applications very very quickly you don't have to use front-end Frameworks such as react.js Etc this Library will allow you\\n\\n00:19:11\\tto do all of these things very very fast so let me just show you so you can just create a simple application with a title rest to run name generator and by the way you have to do pip install pip install streamlit before you start using it otherwise you'll get an error so make sure you have run that and this is the simple app with one title now I can go to terminal and I can just say streamlit run main dot pi and it is going to open up an application in my browser see simple application with this particular\\n\\n00:19:49\\ttitle now I can create the Picker where you can pick the cuisine and for that I'll use the sidebar so in streamlit there is something called sidebar where you can create a select box and give a name to that box so you will say pick a cuisine and give all the options that you want in that particular drop down so I will just put bunch of Cuisines Indian American Mexican and so on and then here you will say if or let me just show you how this looks so just say hit Ctrl s save and click on rerun you can click go here and rerun or\\n\\n00:20:35\\tjust press R key and it will show you see you get this kind of nice picker and if someone picks any entry let's say someone picks Mexican what's gonna happen is that call is going to return that value in a variable which will store in this Cuisine okay so I will just store it in this particular variable and you can say if Cuisine then do something okay what do I want to do I want to generate the restaurant fancy restaurant name and list of manual items here for that let me just write a a dummy code here so I will just call\\n\\n00:21:14\\tthat function let's say get restore name and item where you supply the cuisine as input and it returns let's say rest or a name foreign it is always a good idea to write this kind of stub function or empty function so that you can check your wiring and then you can write the actual code in that function so let's say my restaurant name is curry delight and my menu items is whatever items you like okay so if if Cuisine then get those things as a response and then from the response let's say the restaurant name I can show it here on\\n\\n00:22:10\\tthe right hand side see here somewhere below this header I want to show it and I will use maybe let's say St dot header I will use St dot header as a UI control so let's do this S3 dot header and when I get the menu items so let me get the menu items here so the menu items are going to be this menu items here and whenever you have comma separated string or you can call this split function and specify the separator which is comma here and this should return you a list and once you get that list you can\\n\\n00:23:03\\titerate over that list and you can write those items here [Music] um I can just say item and maybe I can put some character just to indicate this is an item you can also uh all right like kind of like a header where you'll say okay these are menu items manual items okay hit save this code is ready you go back to your UI rerun and see you're getting the restaurant name and the menu items now when you change this selection it's not going to change because obviously we are returning the hard-coded response and the next step is\\n\\n00:23:46\\tto write that code which we wrote in our Jupiter notebook and put that code here okay and since I like to modularize my code I'm going to create a new python file let's call it Lang chain helper and in this file I will copy paste this function and here you can import that module and you just simply call that here see now let's focus on line chain helper so what do we need to do here well the same thing that we did in our notebook so I'm just going to copy paste some code from my notebook here okay we don't need to go over it because\\n\\n00:24:35\\twe have already written that code I will also create a file for my secret key so I will call it secret key and in that secret key file I will place my open AI security now I'm not going to show you my key because of course it's private thing but you will type whatever key you got remember you got five dollar credit so you can do a lot of things with five dollar is more than enough uh so put that that thing here and then you are using that key you are importing that variable from that python file here directly\\n\\n00:25:15\\tokay so my key is ready what else do I need to do well again copy paste business folks copy paste is a boon for any programmer or a data scientist so we just copy pasted the code for the sequential chain that we wrote in our notebook see here we create a restaurant name chain here we create menu item chain and we just return this response folks this is this is so straightforward okay and I have this habit of creating this function main function just so that I can test it so I will say if name is underscore underscore main uh\\n\\n00:26:00\\tthen print generator store name let's say Italian okay and now what I'll do is I'll pause the video and put my real secret key here all right my secret key I have placed it and now I can run it and see what happens so we are generating Italian food restaurant name in the menu items perfect so the restaurant name is La Dolce Vita whatever and menu items are Margarita Pizza alfredo lasagna and all that one thing I'm noticing here is I see some extra slice and characters here so maybe we need to remove them so the way to do\\n\\n00:26:39\\tthat would be let's go back to our streamlit code and instead of just saying responsible restaurant name we can call this stripp function that will remove the leading and trailing white spaces including those slash and characters and you can use that same thing here as well before calling split so hit Ctrl s save let's go back rerun dental and see Italian food this is my restaurant name menu item you can change it to Mexican change it to whatever just play with it and folks The Art of Getting skillful at coding is to\\n\\n00:27:20\\tpractice just by watching this video you're not going to learn it so make sure you're practicing while watching this video all right our streamlit application is ready as a next step we are going to look into something called agents which is a very powerful Concept in Lang chain and by the way all the code that we are writing check video description we are going to give you all of that code if agents is a very powerful Concept in Lang chain what happens when you type this in chatgpt when you say give me two flights\\n\\n00:27:50\\toptions from New York to Delhi on a given date obviously it won't be able to answer because it has knowledge till September 2021 but if you have chat GPD Plus subscription there is this thing called plugins and I have installed those plugins especially the Expedia plugin Expedia as a website which helps you find tickets and when I give the same question now with the plugin enabled magically it will start working so it will go to Xperia plugin try to pull the information on the flights for a given\\n\\n00:28:25\\tdate and given source and destination and then it will start typing those two options see number one option is only I think 500 ticket 518 ticket which is a pretty good deal by the way actually I think I should book it for my next India trip and there is the second option and it will give you a link where you can go and book those tickets on Expedia so what exactly happened when we enable this plugin let's try to understand that so when you think about llm many people think that it is just a knowledge engine\\n\\n00:29:02\\tit has knowledge and it just try to give answer based on that knowledge but the knowledge is only limited to September 2021. the thing that we miss out is it has a reasoning component so it is a reasoning engine too and using that reasoning engine it can figure out that when someone types this kind of question see when as a human when we look at these questions what do we think let's say if we have to go to Wikipedia or not Wikipedia Expedia and if you have to type uh convert this question let's say\\n\\n00:29:34\\tif your friend asks you this question and let's say you are that reasoning engine you go to Expedia and in the source you will put New York destination you will put Delhi date you will put first August how can you do that because you have that reasoning engine in your brain similarly llm has a reasoning engine using which from that sentence it will figure out source is this destination is this that and it will call the Xperia plugin and that will return the response back let's look at some other question when was Elon Musk\\n\\n00:30:05\\tborn what is his age now in 2023 now maybe this can be answered by the llm's knowledge but let's say you are asking some question which is related to an event which happened in 2022 now this guy doesn't have knowledge after September 2021 but once again it has a reasoning capability so it will say okay in order to answer that question first I need to find out when was Elon Musk born for that it can use things like Wikipedia so agents essentially do this thing agents will have tools and using that tool it will try to fetch the\\n\\n00:30:44\\tanswer so Elon Musk was born in 1971 and then there could be another tool which will tell you 2023 minus 1971 how much is that so there is a math tool that it can use to compute that and it will in the end say Elon Musk is 52 year old so this is what agents are agents will connect with external tools it will use llm's reasoning capabilities to perform a given task let's look at a different question how much was U.S GDP in 2022 plus Phi I am doing just like a it's a it's a silly operation no one cares but\\n\\n00:31:24\\tusgd pin 2022 llm doesn't know because its knowledge is still 2021 so it will go to Google it will find that answer and then it will use mat tool to do plus 5. all these tools like Google Search tool map tool and Wikipedia tools are available as part of langchin and you can configure your agent so your agent is nothing but using all these tools and llms reasoning capability to perform a given task that is your agent and this agent can be used in our Jupiter notebook so that's what I'm going to show you next so let's first import\\n\\n00:32:01\\tcouple of important modules and classes and once I've imported them I will create tools so I will say load tools and I will give list of tools now if you do Google search On Tools here so let's say if you do Google Search link chain agent load tools you will come here you will see list of two see I have Wikipedia as a tool I have twilio I have all these tools that I can use so we are going to use Wikipedia tool here it is called Wikipedia we keep it yeah and the math tool is called llm math and here you need to provide the\\n\\n00:32:50\\tllm variable is the one which we created above somewhere here see this is the variable okay so this thing is called tools and then you can create an agent using this initialize agent method okay so initialization method will take tools it will take llm and it will take agent and in the agent I will give this 0 now see hold on zero short react description react means uh thought and action so when we are reasoning we first have a thought then we figure out where to go and we take an action so it mimics\\n\\n00:33:35\\tthat particular concept here I will call this an agent and then I will ask the question agent dot run when was Elon Musk born and what is his age in 2023 so let's see what this gives us see perfect it says 52 year old in 2023 if you want to go uh step by step in the reasoning process you can say verbose is equal to True uh and it will tell you by the way verbose is equal to True is the variable that you can use here in any function to kind of figure out the internal steps that it is taking so here\\n\\n00:34:16\\tthe first step when it Encounters this question it knows actually that it has to go to Wikipedia to get the birth date of Elon Musk so it went to Elon Musk Wikipedia page and which will have this particular date here and then um I think it uses the matte tool sometimes I don't know it should have used the math tool uh Elon was um it doesn't show but if you don't rerun it again okay I don't know why this is not working but previously I was seeing that let me show you a previous snapshot that I have\\n\\n00:34:57\\tum here it says okay went to Wikipedia for Elon musk's birth date and then it use the action as a calculator so there it is using the llm math tool and it is just calculating the final answer it is saying it is 52 year old let's try a different option so this time we are going to use a serp API so if you don't know about serp API it is Google search API whatever you do in Google and what results it gives you if you want to access those results programmatically you can use this particular API you can log in using your\\n\\n00:35:36\\tGmail account I have already logged in and when you go to dashboard it will give you this API key so this is similar to our open AI API it will be a big big string I have stored that API key into my private file that secret key file that I have and I'm going to initialize some environment variable so for serp API you need to initialize this variable and this is the keys which I got from there okay so you can just copy paste this key if you want to keep things simple I don't want to show that key publicly here that's why I have this\\n\\n00:36:10\\tthing here and once I have this thing the next steps are kind of similar so I can just copy paste pretty much everything here and I will just say Okay initialize the agent sir BP and llm math are the two tools I am going to use and in my agent I will say agent dot run and what was the US GDP in 2022 and plus five okay and while it is executing it let's do Google here so when you do Google usdp 2022 it will tell you 25.46 so this serp API this API will do Google search and it will tell you the answer that it is this\\n\\n00:36:58\\tso check this so first it is searching US GDP this and then it is adding Phi number to this and it is giving you uh this particular result we'll talk more about agents in our future videos uh one thing I've noticed is agents are not perfect sometimes they give stupid answer this whole thing is evolving so in the future it will get better but for agent this is what we have and now we will talk about memory when you look at any chat board applications such as chat GPT you will notice that it remembers the past\\n\\n00:37:32\\tconversation Exchange here I asked who won the first Cricket World Cup then a it's totally irrelevant conversation what is five plus five then I am asking who was the captain of the winning team now see here I did not say which match which game cricket football Etc but it remembers that I'm talking about cricket and it is giving me a relevant answer same thing happens with human conversation we start a topic then we keep on saying things but we remember what the topic is about if you look at the llm Chain by default\\n\\n00:38:04\\tThese Chains do not have memory they are stateless and if you look at the available methods that this chain has you'll find that it has an element called memory here see memory uh and if you check the memory so here if you try to print the memory see you don't get anything because the object is set to none now if you want you can attach memory to it so for memory you have to create an additional object and attach it so that it remembers all these conversations this is useful especially if you're\\n\\n00:38:45\\tbuilding a chat board let's say for your customer care Department uh in that many times they need to save their transcripts of those conversations for legal and compliance reasons so here uh I'm going to import an object called conversational buffer memory which is a very common type of memory in line chain module and create an object of that class so I will just say this is a conversational buffer memory and then the same chain I will print here but I will just pass memory as an additional argument\\n\\n00:39:30\\tokay and then I'll run the same chain one more time for a different question now when you look at chain dot memory see there is a memory attached to it we explicitly attach this conversion memory to it and if you look at the buffer and if you just print it for nice alignment Etc see human then AI this human and a address now this looks good you can save this into your database as a saved transcript of your customer service center conversation uh but one problem with this particular object which is\\n\\n00:40:11\\tconversation buffer memory is that it will keep on growing endlessly so let's say you have 100 conversational exchanges and by that what I mean is one question answer pair so one question answer pair is one conversational exchange this is second so in total this is two conversational Exchange so here if you have 100 conversational exchange what's going to happen is next time when you ask a question to open AI when you say chain dot run it is going to send all this past history to open Ai and open AI charges you per token so this is\\n\\n00:40:47\\tone token second token third four and so on for thousand tokens they charge like point zero zero two dollars something this is on the model so your cost is gonna go up so if you want to save the cost and kind of do things in an optimized way uh you need to restrict this buffer size you can say just remember last five conversational exchanges okay and this thing can be done using something called conversation chain so open AI link chain provides this conversation chain which is just very simple object so let me just create\\n\\n00:41:26\\tthat conversation chain where you can just pass llm is equal to open AI temperature is equal to 0.7 and I'll just call it convo and let's check the default prompt that is associated with it see default prompt is this let me just check the template let me just print the template associated with this this is the default template that comes and it says that the following is a friendly conversation between human and Ai and there is history and there is input so if you look at this conversation window here there is a\\n\\n00:42:07\\thistory this is the history and the next question you're going to type is the input so input history same way input history okay now when you ask a bunch of questions to this so I'm just going to copy paste those questions here uh what is five plus five and then who was the captain of the winning team now when you do convert memory it won't be empty because by default conversation chain has this memory associated with it so by default this conversation chain object comes with inbuilt conversation buffer memory and\\n\\n00:42:53\\tif you print the buffer you will see the entire transcript of our conversation now while this looks good once again think about the open AI token cost if this keeps on building the buffer endlessly then you might have 5000 tokens in one conversation and when you make a next call like this is gonna actually send an entire history entire conversation to open Ai and that will increase your bill on the API call to tackle that problem maybe what you can do is you can say just send only last 10 or 20 conversational exchanges\\n\\n00:43:32\\tbecause that's what I care about that might be enough based on the use case that you're dealing with and for that there is an object called from Lang chain dot memory import there is a conversational buffer window memory you are restricted in the window you are saying let's say my window and key is the parameter you're saying just remember only last one conversational exchange which is one question answer pair okay so let's try this out and let's see how this goes so I'm going to copy paste some code here\\n\\n00:44:11\\tcreate a same conversational chain object asking my first question asking my second question now when I ask the second question here what is 5 plus 5 it remembers the previous Exchange but when I I asked the third question here it remembers only what is 5 plus 5 it doesn't remember this it's like a short memory loss like Memento or Disney movie it just forgot what happened here so when I asked this question it will say I'm sorry I don't know because it doesn't know which game you are talking about which particular match you are\\n\\n00:44:47\\ttalking about okay so here I know it's probably not the best example but the idea is I wanted to demonstrate this K parameter here and based on this use case uh you might see benefit in using conversational buffer window memory that's all we had for this video in the future we are going to build end-to-end llm applications using some of the advanced features such as retrieval QA chain the face or DB store Vector store uh things like that so if you want to get notified about upcoming llml line chain videos what you can do is\\n\\n00:45:22\\tsubscribe to the channel or click the like button if you really found some benefit from this video making these kind of videos takes a lot of effort and clicking that thumbs up button or subscribing or sharing this video might be a smaller food for you but it would mean a big thing for us it will help us get this video to more people so that those people can also benefit and we also get some appreciation some reward of the hard work that we are putting and if you have any question Post in the comment box below the code link is given\\n\\n00:45:57\\tin the video description thank you for watching bye bye thank you\\n\\n\", metadata={'source': 'transcription.txt'})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"transcription.txt\")\n",
    "text_documents = loader.load()\n",
    "text_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different ways to split a document. For this example, we'll use a simple splitter that splits the document into chunks of a fixed size. Check [Text Splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/) for more information about different approaches to splitting documents.\n",
    "\n",
    "For illustration purposes, let's split the transcription into chunks of 100 characters with an overlap of 20 characters and display the first few chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='00:00:00\\tLangston is a framework that allows you to build applications on top of llm or large', metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content='top of llm or large language model in this crash course video we are going to go over all the', metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content='to go over all the basics of Lang chain and then we will build a restaurant either generator', metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content='either generator application using streamlit where you can input any cuisine Indian Mexican Etc and', metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content='Mexican Etc and it will generate a fancy restaurant name along with the manual items first let us', metadata={'source': 'transcription.txt'})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "text_splitter.split_documents(text_documents)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our specific application, let's use 1000 characters instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "documents = text_splitter.split_documents(text_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the relevant chunks\n",
    "\n",
    "Given a particular question, we need to find the relevant chunks from the transcription to send to the model. Here is where the idea of **embeddings** comes into play.\n",
    "\n",
    "An embedding is a mathematical representation of the semantic meaning of a word, sentence, or document. It's a projection of a concept in a high-dimensional space. Embeddings have a simple characteristic: The projection of related concepts will be close to each other, while concepts with different meanings will lie far away. You can use the [Cohere's Embed Playground](https://dashboard.cohere.com/playground/embed) to visualize embeddings in two dimensions.\n",
    "\n",
    "To provide with the most relevant chunks, we can use the embeddings of the question and the chunks of the transcription to compute the similarity between them. We can then select the chunks with the highest similarity to the question and use them as the context for the model:\n",
    "\n",
    "<img src='images/system3.png' width=\"1200\">\n",
    "\n",
    "Let's generate embeddings for an arbitrary query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.40.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.11.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.23.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (2021.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.11.0->sentence-transformers) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.11.0->sentence-transformers) (2021.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Goutam.Tak\\AppData\\Local\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.04393356, 0.05893442, 0.04817837, 0.0775481 , 0.02674439],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "# embedded_query = embeddings.embed_query(\"Who is Mary's sister?\")\n",
    "\n",
    "# print(f\"Embedding length: {len(embedded_query)}\")\n",
    "# print(embedded_query[:10])\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embeddings = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Our sentences we like to encode\n",
    "sentences = [\n",
    "    \"This framework generates embeddings for each input sentence\",\n",
    "    \"Sentences are passed as a list of string.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "]\n",
    "\n",
    "# Sentences are encoded by calling model.encode()\n",
    "embedded_query = embeddings.encode(sentences[2])\n",
    "embedded_query[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate how embeddings work, let's first generate the embeddings for two different sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = embeddings.encode(\"quick brown fox jumps over the lazy dog\")\n",
    "sentence2 = embeddings.encode(\"Pedro's mother is a teacher\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute the similarity between the query and each of the two sentences. The closer the embeddings are, the more similar the sentences will be.\n",
    "\n",
    "We can use [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity) to calculate the similarity between the query and each of the sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9685498, 0.0146639)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "query_sentence1_similarity = cosine_similarity([embedded_query], [sentence1])[0][0]\n",
    "query_sentence2_similarity = cosine_similarity([embedded_query], [sentence2])[0][0]\n",
    "\n",
    "query_sentence1_similarity, query_sentence2_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a Vector Store\n",
    "\n",
    "We need an efficient way to store document chunks, their embeddings, and perform similarity searches at scale. To do this, we'll use a **vector store**.\n",
    "\n",
    "A vector store is a database of embeddings that specializes in fast similarity searches. \n",
    "\n",
    "<img src='images/system4.png' width=\"1200\">\n",
    "\n",
    "To understand how a vector store works, let's create one in memory and add a few embeddings to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain[docarray] in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (0.1.20)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for hnswlib (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [5 lines of output]\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_ext\n",
      "  building 'hnswlib' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for hnswlib\n",
      "ERROR: Could not build wheels for hnswlib, which is required to install pyproject.toml-based projects\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain[docarray]) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain[docarray]) (2.0.25)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain[docarray]) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain[docarray]) (0.6.6)\n",
      "Collecting docarray<0.33.0,>=0.32.0 (from docarray[hnswlib]<0.33.0,>=0.32.0; extra == \"docarray\"->langchain[docarray])\n",
      "  Using cached docarray-0.32.1-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.38 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain[docarray]) (0.0.38)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.52 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain[docarray]) (0.1.52)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain[docarray]) (0.0.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain[docarray]) (0.1.59)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain[docarray]) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain[docarray]) (1.10.12)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain[docarray]) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain[docarray]) (8.2.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain[docarray]) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain[docarray]) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain[docarray]) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain[docarray]) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain[docarray]) (1.9.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain[docarray]) (3.21.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain[docarray]) (0.9.0)\n",
      "Requirement already satisfied: orjson>=3.8.2 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from docarray<0.33.0,>=0.32.0->docarray[hnswlib]<0.33.0,>=0.32.0; extra == \"docarray\"->langchain[docarray]) (3.10.3)\n",
      "Requirement already satisfied: rich>=13.1.0 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from docarray<0.33.0,>=0.32.0->docarray[hnswlib]<0.33.0,>=0.32.0; extra == \"docarray\"->langchain[docarray]) (13.3.5)\n",
      "Collecting types-requests>=2.28.11.6 (from docarray<0.33.0,>=0.32.0->docarray[hnswlib]<0.33.0,>=0.32.0; extra == \"docarray\"->langchain[docarray])\n",
      "  Using cached types_requests-2.31.0.20240406-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting hnswlib>=0.6.2 (from docarray[hnswlib]<0.33.0,>=0.32.0; extra == \"docarray\"->langchain[docarray])\n",
      "  Using cached hnswlib-0.8.0.tar.gz (36 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: protobuf>=3.19.0 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from docarray[hnswlib]<0.33.0,>=0.32.0; extra == \"docarray\"->langchain[docarray]) (3.20.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.52->langchain[docarray]) (1.33)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.52->langchain[docarray]) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain[docarray]) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain[docarray]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain[docarray]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain[docarray]) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain[docarray]) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain[docarray]) (3.0.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain[docarray]) (2.1)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from rich>=13.1.0->docarray<0.33.0,>=0.32.0->docarray[hnswlib]<0.33.0,>=0.32.0; extra == \"docarray\"->langchain[docarray]) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from rich>=13.1.0->docarray<0.33.0,>=0.32.0->docarray[hnswlib]<0.33.0,>=0.32.0; extra == \"docarray\"->langchain[docarray]) (2.15.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain[docarray]) (1.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\goutam.tak\\appdata\\local\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=13.1.0->docarray<0.33.0,>=0.32.0->docarray[hnswlib]<0.33.0,>=0.32.0; extra == \"docarray\"->langchain[docarray]) (0.1.0)\n",
      "Using cached docarray-0.32.1-py3-none-any.whl (215 kB)\n",
      "Using cached types_requests-2.31.0.20240406-py3-none-any.whl (15 kB)\n",
      "Building wheels for collected packages: hnswlib\n",
      "  Building wheel for hnswlib (pyproject.toml): started\n",
      "  Building wheel for hnswlib (pyproject.toml): finished with status 'error'\n",
      "Failed to build hnswlib\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain[docarray]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import docarray python package. Please install it with `pip install \"langchain[docarray]\"`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\langchain_community\\vectorstores\\docarray\\base.py:19\u001b[0m, in \u001b[0;36m_check_docarray_import\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdocarray\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     da_version \u001b[38;5;241m=\u001b[39m docarray\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'docarray'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocArrayInMemorySearch\n\u001b[1;32m----> 3\u001b[0m vectorstore1 \u001b[38;5;241m=\u001b[39m DocArrayInMemorySearch\u001b[38;5;241m.\u001b[39mfrom_texts(\n\u001b[0;32m      4\u001b[0m     [\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMary\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms sister is Susana\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJohn and Tommy are brothers\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPatricia likes white cars\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPedro\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms mother is a teacher\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLucia drives an Audi\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMary has two siblings\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m     ],\n\u001b[0;32m     12\u001b[0m     embedding\u001b[38;5;241m=\u001b[39membeddings,\n\u001b[0;32m     13\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\langchain_community\\vectorstores\\docarray\\in_memory.py:68\u001b[0m, in \u001b[0;36mDocArrayInMemorySearch.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, **kwargs)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m     53\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DocArrayInMemorySearch:\n\u001b[0;32m     54\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create an DocArrayInMemorySearch store and insert data.\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03m        DocArrayInMemorySearch Vector Store\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m     store \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_params(embedding, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     69\u001b[0m     store\u001b[38;5;241m.\u001b[39madd_texts(texts\u001b[38;5;241m=\u001b[39mtexts, metadatas\u001b[38;5;241m=\u001b[39mmetadatas)\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m store\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\langchain_community\\vectorstores\\docarray\\in_memory.py:39\u001b[0m, in \u001b[0;36mDocArrayInMemorySearch.from_params\u001b[1;34m(cls, embedding, metric, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_params\u001b[39m(\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m     29\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DocArrayInMemorySearch:\n\u001b[0;32m     30\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize DocArrayInMemorySearch store.\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124;03m        **kwargs: Other keyword arguments to be passed to the get_doc_cls method.\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m     _check_docarray_import()\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindex\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InMemoryExactNNIndex\n\u001b[0;32m     42\u001b[0m     doc_cls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_doc_cls(space\u001b[38;5;241m=\u001b[39mmetric, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\langchain_community\\vectorstores\\docarray\\base.py:29\u001b[0m, in \u001b[0;36m_check_docarray_import\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     24\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use the DocArrayHnswSearch VectorStore the docarray \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     25\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion >=0.32.0 is expected, received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdocarray\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     26\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo upgrade, please run: `pip install -U docarray`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     27\u001b[0m         )\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 29\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import docarray python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     31\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlease install it with `pip install \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlangchain[docarray]\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     32\u001b[0m     )\n",
      "\u001b[1;31mImportError\u001b[0m: Could not import docarray python package. Please install it with `pip install \"langchain[docarray]\"`."
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "vectorstore1 = DocArrayInMemorySearch.from_texts(\n",
    "    [\n",
    "        \"Mary's sister is Susana\",\n",
    "        \"John and Tommy are brothers\",\n",
    "        \"Patricia likes white cars\",\n",
    "        \"Pedro's mother is a teacher\",\n",
    "        \"Lucia drives an Audi\",\n",
    "        \"Mary has two siblings\",\n",
    "    ],\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now query the vector store to find the most similar embeddings to a given query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content=\"Mary's sister is Susana\"), 0.9172681550033158),\n",
       " (Document(page_content='Mary has two siblings'), 0.9045628481161789),\n",
       " (Document(page_content='John and Tommy are brothers'), 0.8015500435454899)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore1.similarity_search_with_score(query=\"Who is Mary's sister?\", k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting the vector store to the chain\n",
    "\n",
    "We can use the vector store to find the most relevant chunks from the transcription to send to the model. Here is how we can connect the vector store to the chain:\n",
    "\n",
    "<img src='images/chain4.png' width=\"1200\">\n",
    "\n",
    "We need to configure a [Retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/). The retriever will run a similarity search in the vector store and return the most similar documents back to the next step in the chain.\n",
    "\n",
    "We can get a retriever directly from the vector store we created before: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Mary's sister is Susana\"),\n",
       " Document(page_content='Mary has two siblings'),\n",
       " Document(page_content='John and Tommy are brothers'),\n",
       " Document(page_content=\"Pedro's mother is a teacher\")]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever1 = vectorstore1.as_retriever()\n",
    "retriever1.invoke(\"Who is Mary's sister?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our prompt expects two parameters, \"context\" and \"question.\" We can use the retriever to find the chunks we'll use as the context to answer the question.\n",
    "\n",
    "We can create a map with the two inputs by using the [`RunnableParallel`](https://python.langchain.com/docs/expression_language/how_to/map) and [`RunnablePassthrough`](https://python.langchain.com/docs/expression_language/how_to/passthrough) classes. This will allow us to pass the context and question to the prompt as a map with the keys \"context\" and \"question.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(page_content='Patricia likes white cars'),\n",
       "  Document(page_content='Lucia drives an Audi'),\n",
       "  Document(page_content=\"Pedro's mother is a teacher\"),\n",
       "  Document(page_content=\"Mary's sister is Susana\")],\n",
       " 'question': \"What color is Patricia's car?\"}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "setup = RunnableParallel(context=retriever1, question=RunnablePassthrough())\n",
    "setup.invoke(\"What color is Patricia's car?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now add the setup map to the chain and run it:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'White'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = setup | prompt | model | parser\n",
    "chain.invoke(\"What color is Patricia's car?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's invoke the chain using another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lucia drives an Audi.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What car does Lucia drive?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading transcription into the vector store\n",
    "\n",
    "We initialized the vector store with a few random strings. Let's create a new vector store using the chunks from the video transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore2 = DocArrayInMemorySearch.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up a new chain using the correct vector store. This time we are using a different equivalent syntax to specify the [`RunnableParallel`](https://python.langchain.com/docs/expression_language/how_to/map) portion of the chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Synthetic intelligence is described as the next stage of development in the context provided. It is mentioned that synthetic intelligences will uncover puzzles in the universe and solve them.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = (\n",
    "    {\"context\": vectorstore2.as_retriever(), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "chain.invoke(\"What is synthetic intelligence?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Pinecone\n",
    "\n",
    "So far we've used an in-memory vector store. In practice, we need a vector store that can handle large amounts of data and perform similarity searches at scale. For this example, we'll use [Pinecone](https://www.pinecone.io/).\n",
    "\n",
    "The first step is to create a Pinecone account, set up an index, get an API key, and set it as an environment variable `PINECONE_API_KEY`.\n",
    "\n",
    "Then, we can load the transcription documents into Pinecone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svpino/dev/youtube-rag/.venv/lib/python3.9/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "index_name = \"youtube-rag-index\"\n",
    "\n",
    "pinecone = PineconeVectorStore.from_documents(\n",
    "    documents, embeddings, index_name=index_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now run a similarity search on pinecone to make sure everything works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"It's like high quality audio and you're speaking usually pretty clearly. I don't know what open AI's plans are either. Yeah, there's always fun projects basically. And stable diffusion also is opening up a huge amount of experimentation. I would say in the visual realm and generating images and videos and movies. I'll think like videos now. And so that's going to be pretty crazy. That's going to almost certainly work and it's going to be really interesting when the cost of content creation is going to fall to zero. You used to need a painter for a few months to paint a thing and now it's going to be speak to your phone to get your video. So Hollywood will start using it to generate scenes, which completely opens up. Yeah, so you can make a movie like Avatar eventually for under a million dollars. Much less. Maybe just by talking to your phone. I mean, I know it sounds kind of crazy. And then there'd be some voting mechanism. Like how do you have a, like, would there be a show on\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"get to another celebrity and might get into other big accounts. And then it'll just, so with just that simple goal, get them to respond. Yeah. Maximize the probability of actual response. Yeah, I mean, you could prompt a powerful model like this with their, it's opinion about how to do any possible thing you're interested in. So they will check us. They're kind of on track to become these oracles. I could sort of think of it that way. They are oracles currently is just text, but they will have calculators, they will have access to Google search, they will have all kinds of gadgets and gizmos, they will be able to operate the internet and find different information. And yeah, in some sense, that's kind of like currently what it looks like in terms of the development. Do you think it'll be an improvement eventually over what Google is for access to human knowledge? Like it'll be a more effective search engine to access human knowledge. I think there's definitely scope in building a\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"space but in the digital space it just feels like it's going to be very tricky. Very tricky to out because it seems to be pretty low cost to fake stuff. What are you going to put an AI in jail for like trying to use a fake personhood proof? I mean okay fine you'll put a lot of AI in jail but there'll be more AI's like exponentially more. The cost of creating bought is very low. Unless there's some kind of way to track accurately like you're not allowed to create any program without showing tying yourself to that program. Like any program that runs on the internet you'll be able to trace every single human program that was involved with that program. Yeah maybe you have to start declaring when you know we have to start drawing those boundaries and keeping track of okay what are digital entities versus human entities and what is the ownership of human entities and digital entities and something like that. I don't know but I think I'm optimistic that this is possible and in some sense\", metadata={'source': 'transcription.txt'})]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone.similarity_search(\"What is Hollywood going to start doing?\")[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's setup the new chain using Pinecone as the vector store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hollywood is going to start using AI to generate scenes for movies.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = (\n",
    "    {\"context\": pinecone.as_retriever(), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "chain.invoke(\"What is Hollywood going to start doing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
